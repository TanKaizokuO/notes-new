---
tags:
  - umbrella
---
---

# ğŸ—‚ï¸ NLP & Data Representation Index

Welcome to the knowledge base for Natural Language Processing data representation. This index organizes notes from raw data preprocessing to advanced deep learning embeddings.

---

## ğŸ—ï¸ Foundations
Before processing text, we must understand how to handle data and the general pipeline.
* [[Categorical Encoding]] â€” Handling non-text categorical data (One-Hot vs. Ordinal).
* [[Text Representation]] â€” The high-level pipeline: converting text to numbers.

---

## ğŸ§® Vectorization Techniques
Techniques to convert text into numerical vectors.

### 1. Sparse / Counting Methods
Simple, interpretable, but high-dimensional approaches.
* [[Basic Vectorization Approaches]] â€” Covers One-Hot, Bag of Words (BoW), and TF-IDF.

### 2. Dense / Deep Learning Methods
Advanced approaches that capture semantic meaning.
* [[Distributed Representation (Embeddings)]] â€” The concept of dense vectors and the distributional hypothesis.

---

## ğŸ¤– Embedding Models
Specific architectures used to learn distributed representations.
* [[Word2Vec]] â€” The family of models including **CBOW** and **Skip-gram**, utilizing **Negative Sampling**.
* [[GloVe]] â€” Global Vectors for Word Representation (Matrix Factorization + Context Window).

---

## ğŸ“ Core Concepts & Math
The mathematical principles that make embeddings work.
* [[Cosine Similarity]] â€” How we measure distance/similarity between word vectors.
* [[Word Analogy Reasoning]] â€” How algebra ($King - Man + Woman \approx Queen$) works in vector space.

---

## ğŸš€ Workflow & Applications
Practical usage and ethical considerations.
* [[NLP and Word Embeddings Workflow]] â€” The process of Pre-training, Transfer Learning, and Visualization (t-SNE).
* [[Sentiment Classification with Embeddings]] â€” Using embeddings for tasks like review analysis.
* [[Debiasing Word Embeddings]] â€” Identifying and removing human bias (gender, race) from models.