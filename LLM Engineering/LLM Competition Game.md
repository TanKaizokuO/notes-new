---
tags:
  - main
---
---

A competitive benchmarking framework for LLMs.

## Steps
1.  **Input**: Use the same prompt.
2.  **Run**: Query multiple models.
3.  **Eval**: Score the outputs.

## Dimensions to Score
* Reasoning
* Creativity
* Speed
* Cost

## Purpose
* Model selection
* Prompt optimization
* Understanding bias

> [!TIP]
> Automate this process using scripts and APIs for efficiency.

**Links**:
* [[Testing Frontier LLMs]]